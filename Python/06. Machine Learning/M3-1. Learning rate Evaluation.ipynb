{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate 에 따른 학습\n",
    "> * Case1 : learning_rate = 0.1 \n",
    "* Case2 : learning_rate = 1.5\n",
    "* Case3 : learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1 : learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.math.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ea5e92a8bc45c0a4e89d774ab0f3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 3.0203847885131836 \n",
      " Weight : \n",
      "[[-0.17754334 -0.78997195 -0.04304576]\n",
      " [-1.5027626   0.48516145 -1.071294  ]\n",
      " [ 0.30098546 -0.8247319  -0.01561223]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : 1.216780662536621 \n",
      " Weight : \n",
      "[[-0.4375412  -0.77220863  0.19918884]\n",
      " [-1.0890027  -0.05018669 -0.9497054 ]\n",
      " [ 0.1900083  -0.5525086  -0.17685825]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : 0.9577064514160156 \n",
      " Weight : \n",
      "[[-0.6784514  -0.7165572   0.38444754]\n",
      " [-0.85323054 -0.36418664 -0.8714775 ]\n",
      " [ 0.05116941 -0.23401435 -0.35651356]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : 0.8223516941070557 \n",
      " Weight : \n",
      "[[-0.8782285  -0.6705325   0.5381999 ]\n",
      " [-0.70902747 -0.5678387  -0.81202847]\n",
      " [-0.01299538 -0.03458347 -0.4917797 ]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : 0.7558510899543762 \n",
      " Weight : \n",
      "[[-1.043373   -0.6384493   0.6712613 ]\n",
      " [-0.63382375 -0.6737964  -0.7812743 ]\n",
      " [-0.02247061  0.06562337 -0.5825114 ]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : 0.7156873941421509 \n",
      " Weight : \n",
      "[[-1.1844605  -0.6150593   0.7889591 ]\n",
      " [-0.5953213  -0.72242665 -0.77114636]\n",
      " [-0.00510452  0.11020622 -0.6444603 ]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : 0.6862089037895203 \n",
      " Weight : \n",
      "[[-1.3092692  -0.59600514  0.8947133 ]\n",
      " [-0.5744009  -0.742538   -0.77195567]\n",
      " [ 0.02321454  0.12751576 -0.6900888 ]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : 0.6623257398605347 \n",
      " Weight : \n",
      "[[-1.4225476  -0.5790013   0.9909878 ]\n",
      " [-0.5620909  -0.74903035 -0.77777314]\n",
      " [ 0.05544377  0.13175201 -0.72655445]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : 0.6420379877090454 \n",
      " Weight : \n",
      "[[-1.5272138  -0.56295013  1.0796026 ]\n",
      " [-0.5542566  -0.74915844 -0.7854791 ]\n",
      " [ 0.08862306  0.12976782 -0.7577495 ]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : 0.6243385076522827 \n",
      " Weight : \n",
      "[[-1.6251488  -0.54734087  1.1619279 ]\n",
      " [-0.5489345  -0.74643195 -0.7935276 ]\n",
      " [ 0.12154441  0.12488233 -0.7857852 ]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : 0.6086158752441406 \n",
      " Weight : \n",
      "[[-1.7176317  -0.5319456   1.2390152 ]\n",
      " [-0.54514915 -0.7425432  -0.8012017 ]\n",
      " [ 0.1537489   0.11870962 -0.81181675]]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [2 2 2]\n",
      "Accuracy \t:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in tqdm_notebook(range(201)):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2 : learning_rate = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 1.5\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 3.945857524871826 \n",
      " Weight : \n",
      "[[-0.2368362   0.6412566  -0.09345132]\n",
      " [-2.582366    4.519682    1.7531003 ]\n",
      " [-2.9232774   1.2354764   1.9879432 ]]\n",
      "\n",
      " Step : 1 \n",
      " Cost : 21.469520568847656 \n",
      " Weight : \n",
      "[[ 0.1381638  -0.28146142  0.45426673]\n",
      " [-0.14486599  0.6122863   3.2229962 ]\n",
      " [-0.48577738 -2.4441977   3.230117  ]]\n",
      "\n",
      " Step : 2 \n",
      " Cost : 28.177921295166016 \n",
      " Weight : \n",
      "[[ 0.5131634   0.28103817 -0.48323244]\n",
      " [ 2.2926333   3.2372854  -1.8395019 ]\n",
      " [ 1.9517221   0.36830187 -2.019882  ]]\n",
      "\n",
      " Step : 3 \n",
      " Cost : 9.0123291015625 \n",
      " Weight : \n",
      "[[-0.24094301  0.47264472  0.07926732]\n",
      " [-0.50024056  4.5301595  -0.33950233]\n",
      " [-1.2259152   2.2334397  -0.7073822 ]]\n",
      "\n",
      " Step : 4 \n",
      " Cost : 21.952831268310547 \n",
      " Weight : \n",
      "[[ 0.13405696 -0.46485475  0.6417668 ]\n",
      " [ 1.9372594   0.5926604   1.1604966 ]\n",
      " [ 1.2115848  -1.5165596   0.6051173 ]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [0 0 0]\n",
      "Accuracy \t:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0 or step<5:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case3 : learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 6.003984451293945 \n",
      " Weight : \n",
      "[[ 0.04672125 -2.214816    1.7723497 ]\n",
      " [-2.3809586   0.26667655 -0.5788484 ]\n",
      " [-1.046504    0.05560379 -0.857442  ]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : 6.000621795654297 \n",
      " Weight : \n",
      "[[ 0.04677109 -2.2149115   1.772395  ]\n",
      " [-2.3806343   0.26622456 -0.57872087]\n",
      " [-1.0461798   0.05515679 -0.8573201 ]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : 5.997260093688965 \n",
      " Weight : \n",
      "[[ 0.04682093 -2.2150068   1.7724403 ]\n",
      " [-2.38031     0.26577276 -0.5785933 ]\n",
      " [-1.0458555   0.0547099  -0.8571985 ]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : 5.993899345397949 \n",
      " Weight : \n",
      "[[ 0.04687078 -2.2151022   1.7724832 ]\n",
      " [-2.3799858   0.26532096 -0.57846576]\n",
      " [-1.0455313   0.05426313 -0.8570769 ]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : 5.990540504455566 \n",
      " Weight : \n",
      "[[ 0.04692062 -2.2151976   1.7725261 ]\n",
      " [-2.3796616   0.26486915 -0.57833844]\n",
      " [-1.045207    0.05381648 -0.8569553 ]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : 5.987183570861816 \n",
      " Weight : \n",
      "[[ 0.04697047 -2.215293    1.7725691 ]\n",
      " [-2.3793373   0.26441735 -0.5782121 ]\n",
      " [-1.0448828   0.05336994 -0.8568337 ]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : 5.983827590942383 \n",
      " Weight : \n",
      "[[ 0.04702031 -2.2153883   1.772612  ]\n",
      " [-2.379013    0.26396587 -0.5780857 ]\n",
      " [-1.0445585   0.05292353 -0.8567121 ]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : 5.980473518371582 \n",
      " Weight : \n",
      "[[ 0.04707016 -2.2154837   1.7726549 ]\n",
      " [-2.3786888   0.26351467 -0.57795936]\n",
      " [-1.0442343   0.05247723 -0.8565905 ]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : 5.977120876312256 \n",
      " Weight : \n",
      "[[ 0.04712    -2.215579    1.7726978 ]\n",
      " [-2.3783646   0.26306346 -0.577833  ]\n",
      " [-1.04391     0.05203105 -0.8564689 ]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : 5.973769187927246 \n",
      " Weight : \n",
      "[[ 0.04716985 -2.2156744   1.7727407 ]\n",
      " [-2.3780403   0.26261225 -0.57770663]\n",
      " [-1.0435858   0.05158499 -0.8563473 ]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : 5.970418930053711 \n",
      " Weight : \n",
      "[[ 0.04721969 -2.2157698   1.7727836 ]\n",
      " [-2.377716    0.26216105 -0.5775803 ]\n",
      " [-1.0432615   0.05113904 -0.8562257 ]]\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [2 2 2]\n",
      "Accuracy \t:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalized inputs\n",
    "> Linear regression without min, max\n",
    "* 너무나 심플하고 멋진 모델임에도 불구하고 학습해보면 NaN 이 발생\n",
    "* 해결방안 : Nomalized 한다거나, MinMaxScaler 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_data : \n",
      "[[8.28659973e+02 8.33450012e+02 9.08100000e+05 8.28349976e+02]\n",
      " [8.23020020e+02 8.28070007e+02 1.82810000e+06 8.21655029e+02]\n",
      " [8.19929993e+02 8.24400024e+02 1.43810000e+06 8.18979980e+02]\n",
      " [8.16000000e+02 8.20958984e+02 1.00810000e+06 8.15489990e+02]\n",
      " [8.19359985e+02 8.23000000e+02 1.18810000e+06 8.18469971e+02]\n",
      " [8.19000000e+02 8.23000000e+02 1.19810000e+06 8.16000000e+02]\n",
      " [8.11700012e+02 8.15250000e+02 1.09810000e+06 8.09780029e+02]\n",
      " [8.09510010e+02 8.16659973e+02 1.39810000e+06 8.04539978e+02]]  \n",
      "\n",
      " y_data : \n",
      "[[831.659973]\n",
      " [828.070007]\n",
      " [824.159973]\n",
      " [819.23999 ]\n",
      " [818.97998 ]\n",
      " [820.450012]\n",
      " [813.669983]\n",
      " [809.559998]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100,  828.349976, 831.659973],\n",
    "               [823.02002,  828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998,  824.159973],\n",
    "               [816,        820.958984, 1008100, 815.48999,  819.23999 ],\n",
    "               [819.359985, 823,        1188100, 818.469971, 818.97998 ],\n",
    "               [819,        823,        1198100, 816,        820.450012],\n",
    "               [811.700012, 815.25,     1098100, 809.780029, 813.669983],\n",
    "               [809.51001,  816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(\" x_data : \\n{}  \\n\\n y_data : \\n{}\".format(x_data, y_data))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0, \t Cost : 1288165261312.0\n",
      "Step : 1, \t Cost : 1.4152820192956359e+27\n",
      "Step : 2, \t Cost : inf\n",
      "Step : 3, \t Cost : inf\n",
      "Step : 4, \t Cost : inf\n",
      "Step : 5, \t Cost : inf\n",
      "Step : 6, \t Cost : nan\n",
      "Step : 7, \t Cost : nan\n",
      "Step : 8, \t Cost : nan\n",
      "Step : 9, \t Cost : nan\n",
      "Step : 10, \t Cost : nan\n",
      "Step : 11, \t Cost : nan\n",
      "Step : 12, \t Cost : nan\n",
      "Step : 13, \t Cost : nan\n",
      "Step : 14, \t Cost : nan\n",
      "Step : 15, \t Cost : nan\n",
      "Step : 16, \t Cost : nan\n",
      "Step : 17, \t Cost : nan\n",
      "Step : 18, \t Cost : nan\n",
      "Step : 19, \t Cost : nan\n",
      "Step : 20, \t Cost : nan\n",
      "Step : 21, \t Cost : nan\n",
      "Step : 22, \t Cost : nan\n",
      "Step : 23, \t Cost : nan\n",
      "Step : 24, \t Cost : nan\n",
      "Step : 25, \t Cost : nan\n",
      "Step : 26, \t Cost : nan\n",
      "Step : 27, \t Cost : nan\n",
      "Step : 28, \t Cost : nan\n",
      "Step : 29, \t Cost : nan\n",
      "Step : 30, \t Cost : nan\n",
      "Step : 31, \t Cost : nan\n",
      "Step : 32, \t Cost : nan\n",
      "Step : 33, \t Cost : nan\n",
      "Step : 34, \t Cost : nan\n",
      "Step : 35, \t Cost : nan\n",
      "Step : 36, \t Cost : nan\n",
      "Step : 37, \t Cost : nan\n",
      "Step : 38, \t Cost : nan\n",
      "Step : 39, \t Cost : nan\n",
      "Step : 40, \t Cost : nan\n",
      "Step : 41, \t Cost : nan\n",
      "Step : 42, \t Cost : nan\n",
      "Step : 43, \t Cost : nan\n",
      "Step : 44, \t Cost : nan\n",
      "Step : 45, \t Cost : nan\n",
      "Step : 46, \t Cost : nan\n",
      "Step : 47, \t Cost : nan\n",
      "Step : 48, \t Cost : nan\n",
      "Step : 49, \t Cost : nan\n",
      "Step : 50, \t Cost : nan\n",
      "Step : 51, \t Cost : nan\n",
      "Step : 52, \t Cost : nan\n",
      "Step : 53, \t Cost : nan\n",
      "Step : 54, \t Cost : nan\n",
      "Step : 55, \t Cost : nan\n",
      "Step : 56, \t Cost : nan\n",
      "Step : 57, \t Cost : nan\n",
      "Step : 58, \t Cost : nan\n",
      "Step : 59, \t Cost : nan\n",
      "Step : 60, \t Cost : nan\n",
      "Step : 61, \t Cost : nan\n",
      "Step : 62, \t Cost : nan\n",
      "Step : 63, \t Cost : nan\n",
      "Step : 64, \t Cost : nan\n",
      "Step : 65, \t Cost : nan\n",
      "Step : 66, \t Cost : nan\n",
      "Step : 67, \t Cost : nan\n",
      "Step : 68, \t Cost : nan\n",
      "Step : 69, \t Cost : nan\n",
      "Step : 70, \t Cost : nan\n",
      "Step : 71, \t Cost : nan\n",
      "Step : 72, \t Cost : nan\n",
      "Step : 73, \t Cost : nan\n",
      "Step : 74, \t Cost : nan\n",
      "Step : 75, \t Cost : nan\n",
      "Step : 76, \t Cost : nan\n",
      "Step : 77, \t Cost : nan\n",
      "Step : 78, \t Cost : nan\n",
      "Step : 79, \t Cost : nan\n",
      "Step : 80, \t Cost : nan\n",
      "Step : 81, \t Cost : nan\n",
      "Step : 82, \t Cost : nan\n",
      "Step : 83, \t Cost : nan\n",
      "Step : 84, \t Cost : nan\n",
      "Step : 85, \t Cost : nan\n",
      "Step : 86, \t Cost : nan\n",
      "Step : 87, \t Cost : nan\n",
      "Step : 88, \t Cost : nan\n",
      "Step : 89, \t Cost : nan\n",
      "Step : 90, \t Cost : nan\n",
      "Step : 91, \t Cost : nan\n",
      "Step : 92, \t Cost : nan\n",
      "Step : 93, \t Cost : nan\n",
      "Step : 94, \t Cost : nan\n",
      "Step : 95, \t Cost : nan\n",
      "Step : 96, \t Cost : nan\n",
      "Step : 97, \t Cost : nan\n",
      "Step : 98, \t Cost : nan\n",
      "Step : 99, \t Cost : nan\n",
      "Step : 100, \t Cost : nan\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"Step : {}, \\t Cost : {}\".format(step, cost_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4).reshape((2,2))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data-np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    print(\" max.data :\\n {max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손글씨 숫자데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)  \n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training epoch / batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f20a16510a14afdb75114e47fe6fa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\tCost = 2.791586801\n",
      "Epoch: 0002\tCost = 1.072621038\n",
      "Epoch: 0003\tCost = 0.848396162\n",
      "Epoch: 0004\tCost = 0.743733543\n",
      "Epoch: 0005\tCost = 0.679391309\n",
      "Epoch: 0006\tCost = 0.633294273\n",
      "Epoch: 0007\tCost = 0.598677966\n",
      "Epoch: 0008\tCost = 0.570822526\n",
      "Epoch: 0009\tCost = 0.548126380\n",
      "Epoch: 0010\tCost = 0.528871997\n",
      "Epoch: 0011\tCost = 0.512251964\n",
      "Epoch: 0012\tCost = 0.497253384\n",
      "Epoch: 0013\tCost = 0.484525664\n",
      "Epoch: 0014\tCost = 0.473563450\n",
      "Epoch: 0015\tCost = 0.462728269\n",
      "\n",
      "Learning finished\n",
      "----------------------------------------------------------------\n",
      "Accuracy:  0.8893\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in tqdm_notebook(range(training_epochs)):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        # print('Epoch:', '%04d' % (epoch + 1),\n",
    "        #       '\\t cost =', '{:.9f}'.format(avg_cost))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), end='\\t')\n",
    "        print('Cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "    print(\"-\"*2**6)\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
